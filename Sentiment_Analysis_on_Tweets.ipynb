{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis-with-Airline-Tweets\" data-toc-modified-id=\"Sentiment-Analysis-with-Airline-Tweets-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sentiment Analysis with Airline Tweets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-analysis\" data-toc-modified-id=\"Data-analysis-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data analysis</a></span></li><li><span><a href=\"#Data-Engineering\" data-toc-modified-id=\"Data-Engineering-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Engineering</a></span></li><li><span><a href=\"#Algorithm-Testing\" data-toc-modified-id=\"Algorithm-Testing-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Algorithm Testing</a></span></li><li><span><a href=\"#Combining-Algorithms-(Ensemble)\" data-toc-modified-id=\"Combining-Algorithms-(Ensemble)-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Combining Algorithms (Ensemble)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Airline Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are many ways to do natural word processing, I decided to try a different approach from the one I tried for the fake news filter. \n",
    "\n",
    "In this kernel, I will be using the NLTK method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, importing the relevant libaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, NaiveBayesClassifier, classify, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tweets.csv', encoding = 'latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also be making a copy of the data to prevent confusion and make clearing of mistakes easier as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()\n",
    "\n",
    "# Then, extract the relevant columns, in this case, the tweet column and the column which determines the sentiment of the tweet \n",
    "\n",
    "main_data = data_copy[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a look at the first few rows of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0               =@VirginAmerica What @dhepburn said.           neutral\n",
       "1  @VirginAmerica plus you've added commercials t...          positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check how many tweets are there and finding if there are any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14639</td>\n",
       "      <td>14639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>14418</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>=@united Thanks</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>9178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text airline_sentiment\n",
       "count             14639             14639\n",
       "unique            14418                 3\n",
       "top     =@united Thanks          negative\n",
       "freq                  6              9178"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14639 entries, 0 to 14638\n",
      "Data columns (total 2 columns):\n",
      "text                 14639 non-null object\n",
      "airline_sentiment    14639 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.8+ KB\n"
     ]
    }
   ],
   "source": [
    "main_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is clean, the next step will be to extract the tweets from the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = main_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     =@VirginAmerica What @dhepburn said.\n",
       "1        @VirginAmerica plus you've added commercials t...\n",
       "2        @VirginAmerica I didn't today... Must mean I n...\n",
       "3        @VirginAmerica it's really aggressive to blast...\n",
       "4        @VirginAmerica and it's a really big bad thing...\n",
       "5        @VirginAmerica seriously would pay $30 a fligh...\n",
       "6        @VirginAmerica yes, nearly every time I fly VX...\n",
       "7        @VirginAmerica Really missed a prime opportuni...\n",
       "8        @virginamerica Well, I didn'tâ¦but NOW I DO! :-D\n",
       "9        @VirginAmerica it was amazing, and arrived an ...\n",
       "10       @VirginAmerica did you know that suicide is th...\n",
       "11       @VirginAmerica I &lt;3 pretty graphics. so muc...\n",
       "12       @VirginAmerica This is such a great deal! Alre...\n",
       "13       @VirginAmerica @virginmedia I'm flying your #f...\n",
       "14                                  @VirginAmerica Thanks!\n",
       "15          =@VirginAmerica SFO-PDX schedule is still MIA.\n",
       "16       @VirginAmerica So excited for my first cross c...\n",
       "17       @VirginAmerica  I flew from NYC to SFO last we...\n",
       "18              I â¤ï¸ flying @VirginAmerica. âºï¸ð\n",
       "19       @VirginAmerica you know what would be amazingl...\n",
       "20       =@VirginAmerica why are your first fares in Ma...\n",
       "21       @VirginAmerica I love this graphic. http://t.c...\n",
       "22       =@VirginAmerica I love the hipster innovation....\n",
       "23       @VirginAmerica will you be making BOS&gt;LAS n...\n",
       "24       @VirginAmerica you guys messed up my seating.....\n",
       "25       @VirginAmerica status match program.  I applie...\n",
       "26       @VirginAmerica What happened 2 ur vegan food o...\n",
       "27       @VirginAmerica do you miss me? Don't worry we'...\n",
       "28       @VirginAmerica amazing to me that we can't get...\n",
       "29       @VirginAmerica LAX to EWR - Middle seat on a r...\n",
       "                               ...                        \n",
       "14609    @AmericanAir I understand the weather issue bu...\n",
       "14610    @AmericanAir guarantee no retribution? If so, ...\n",
       "14611    @AmericanAir a friend is having flight Cancell...\n",
       "14612    @AmericanAir I used the \"call back\" feature wi...\n",
       "14613    @AmericanAir I need to be at work tomorrow at ...\n",
       "14614    @AmericanAir  ugh Dump us in dfw w/no luggage ...\n",
       "14615    @AmericanAir Cancelled Flights my flight, does...\n",
       "14616              =@AmericanAir DMing you now!Big Thanks.\n",
       "14617    @AmericanAir 3078 is overweight so you pull 2 ...\n",
       "14618    =@AmericanAir I love your company and your sta...\n",
       "14619    @AmericanAir I wait 2+ hrs for CS to call me b...\n",
       "14620    @AmericanAir I've been on hold for 55 mins abo...\n",
       "14621    I just need a place to sleep when I land witho...\n",
       "14622    @AmericanAir Love the new planes for the JFK-L...\n",
       "14623    =@AmericanAir call me chairman, Or call me Eme...\n",
       "14624    @AmericanAir Flight 236 was great. Fantastic c...\n",
       "14625    @AmericanAir Flight 953 NYC-Buenos Aires has b...\n",
       "14626    @AmericanAir Flight Cancelled Flightled, can't...\n",
       "14627    Thank you. â@AmericanAir: @jlhalldc Customer...\n",
       "14628    =@AmericanAir how do I change my flight if the...\n",
       "14629                          =@AmericanAir Thanks!He is.\n",
       "14630    =@AmericanAir thx for nothing on getting US ou...\n",
       "14631    â@AmericanAir: @TilleyMonsta George, that do...\n",
       "14632    =@AmericanAir my flight was Cancelled Flightle...\n",
       "14633       =@AmericanAir right on cue with the delaysð\n",
       "14634    =@AmericanAir Thank you we got on a different ...\n",
       "14635    @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14636    @AmericanAir Please bring American Airlines to...\n",
       "14637    @AmericanAir you have my money, you change my ...\n",
       "14638    @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "Name: text, Length: 14639, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "As seen from the tweets, there are many additional characters and letters which will complicate the model and lower the accuracy. As such, I will be removing the irrelevant letters and special characters in the next step. Each step will be explained in further detail as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the processed tweets\n",
    "processed_tweets = []\n",
    "\n",
    "# To remove unnecessary words, characters and spaces, re order them too\n",
    "\n",
    "for each_tweet in range(0, len(tweets)):\n",
    "    # First step involves removing the special characters\n",
    "    processed_tweet = re.sub(r'\\W', ' ', str(tweets[each_tweet]))\n",
    "\n",
    "    # Then, single characters will be removed\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_tweet = re.sub(r'\\s+', ' ', processed_tweet, flags = re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_tweet = processed_tweet.lower()\n",
    "\n",
    "    # Store the processed tweet into the list created above by appending it\n",
    "    processed_tweets.append(processed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to take out the common words and lemmatisation. Common words in this scenario will refer to words such as 'I', 'he' and many others where they do not really provide context to the sentiment analysis. As such, such common words will be removed to improve the efficiency of the algorithms later.\n",
    "\n",
    "A brief description of lemmatisation involves classifying words that have approximately the same meaning but with different spellings as the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the bag of words model\n",
    "CommonWords = stopwords.words('english')\n",
    "wordLemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(0, len(processed_tweets)):\n",
    "    #reading each text\n",
    "    tweet = processed_tweets[i]\n",
    "    #lemmatizing each word of the text. When we tokeninze a sentence we get individual words \n",
    "    wordtokens = [wordLemmatizer.lemmatize(word.lower()) for word in word_tokenize(tweet)] \n",
    "    #filtering out the stopwords from the text and combining them into a list again.\n",
    "    text = ' '.join([x for x in wordtokens if x not in CommonWords])\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the corpus array to see how they have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virginamerica dhepburn said', 'virginamerica plus added commercial experience tacky', 'virginamerica today must mean need take another trip', 'virginamerica really aggressive blast obnoxious entertainment guest face amp little recourse', 'virginamerica really big bad thing']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#creating the sparse matrix\n",
    "cv = CountVectorizer(max_features = 5000)  \n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "y = main_data.iloc[:,1].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df = 0.7)\n",
    "# tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "# tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1643  131   45]\n",
      " [ 229  342   60]\n",
      " [  90   65  323]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.84      0.90      0.87      1819\n",
      "    neutral       0.64      0.54      0.59       631\n",
      "   positive       0.75      0.68      0.71       478\n",
      "\n",
      "avg / total       0.78      0.79      0.78      2928\n",
      "\n",
      "0.7882513661202186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(solver = 'liblinear')\n",
    "LR.fit(x_train, y_train)\n",
    "y_pred_LR = LR.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_LR))\n",
    "print(classification_report(y_test, y_pred_LR))\n",
    "print(accuracy_score(y_test, y_pred_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1604  189   26]\n",
      " [ 287  310   34]\n",
      " [ 140   95  243]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.88      0.83      1819\n",
      "    neutral       0.52      0.49      0.51       631\n",
      "   positive       0.80      0.51      0.62       478\n",
      "\n",
      "avg / total       0.73      0.74      0.73      2928\n",
      "\n",
      "0.7366803278688525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "PAC = PassiveAggressiveClassifier()\n",
    "PAC.fit(x_train, y_train)\n",
    "y_pred_PAC = PAC.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_PAC))\n",
    "print(classification_report(y_test, y_pred_PAC))\n",
    "print(accuracy_score(y_test, y_pred_PAC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1613  136   70]\n",
      " [ 272  305   54]\n",
      " [ 101   64  313]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.81      0.89      0.85      1819\n",
      "    neutral       0.60      0.48      0.54       631\n",
      "   positive       0.72      0.65      0.68       478\n",
      "\n",
      "avg / total       0.75      0.76      0.75      2928\n",
      "\n",
      "0.7619535519125683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(x_train, y_train)\n",
    "y_pred_MNB = MNB.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_MNB))\n",
    "print(classification_report(y_test, y_pred_MNB))\n",
    "print(accuracy_score(y_test, y_pred_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[871 386 562]\n",
      " [107 200 324]\n",
      " [ 90  68 320]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.82      0.48      0.60      1819\n",
      "    neutral       0.31      0.32      0.31       631\n",
      "   positive       0.27      0.67      0.38       478\n",
      "\n",
      "avg / total       0.62      0.48      0.50      2928\n",
      "\n",
      "0.475068306010929\n"
     ]
    }
   ],
   "source": [
    "GNB = GaussianNB()\n",
    "GNB.fit(x_train, y_train)\n",
    "y_pred_GNB = GNB.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_GNB))\n",
    "print(classification_report(y_test, y_pred_GNB))\n",
    "print(accuracy_score(y_test, y_pred_GNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1602  153   64]\n",
      " [ 231  345   55]\n",
      " [  90   76  312]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.83      0.88      0.86      1819\n",
      "    neutral       0.60      0.55      0.57       631\n",
      "   positive       0.72      0.65      0.69       478\n",
      "\n",
      "avg / total       0.77      0.77      0.77      2928\n",
      "\n",
      "0.7715163934426229\n"
     ]
    }
   ],
   "source": [
    "BNB = BernoulliNB()\n",
    "BNB.fit(x_train, y_train)\n",
    "y_pred_BNB = BNB.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_BNB))\n",
    "print(classification_report(y_test, y_pred_BNB))\n",
    "print(accuracy_score(y_test, y_pred_BNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1442  267  110]\n",
      " [ 278  279   74]\n",
      " [ 126   83  269]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.78      0.79      0.79      1819\n",
      "    neutral       0.44      0.44      0.44       631\n",
      "   positive       0.59      0.56      0.58       478\n",
      "\n",
      "avg / total       0.68      0.68      0.68      2928\n",
      "\n",
      "0.6796448087431693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(x_train,y_train)\n",
    "y_pred_DT = DT.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_DT))\n",
    "print(classification_report(y_test, y_pred_DT))\n",
    "print(accuracy_score(y_test, y_pred_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1594  163   62]\n",
      " [ 284  285   62]\n",
      " [ 144   86  248]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.88      0.83      1819\n",
      "    neutral       0.53      0.45      0.49       631\n",
      "   positive       0.67      0.52      0.58       478\n",
      "\n",
      "avg / total       0.71      0.73      0.72      2928\n",
      "\n",
      "0.7264344262295082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifierRF = RandomForestClassifier()\n",
    "classifierRF.fit(x_train, y_train)\n",
    "y_pred_RF = classifierRF.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_RF))\n",
    "print(classification_report(y_test, y_pred_RF))\n",
    "print(accuracy_score(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1681   80   58]\n",
      " [ 331  246   54]\n",
      " [ 126   66  286]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.79      0.92      0.85      1819\n",
      "    neutral       0.63      0.39      0.48       631\n",
      "   positive       0.72      0.60      0.65       478\n",
      "\n",
      "avg / total       0.74      0.76      0.74      2928\n",
      "\n",
      "0.7558060109289617\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGB = XGBClassifier()\n",
    "XGB.fit(x_train,y_train)\n",
    "y_pred_XGB = XGB.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_XGB))\n",
    "print(classification_report(y_test, y_pred_XGB))\n",
    "print(accuracy_score(y_test, y_pred_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive Aggressive Classifier</th>\n",
       "      <td>0.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy Scores\n",
       "Model                                         \n",
       "Multinomial Naive Bayes                  0.617\n",
       "Logistic Regression                      0.616\n",
       "XGBoost                                  0.610\n",
       "Bernoulli Naive Bayes                    0.609\n",
       "Random Forest                            0.576\n",
       "Passive Aggressive Classifier            0.507\n",
       "Decision Tree                            0.467\n",
       "Gaussian Naive Bayes                     0.302"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame({'Model': ['Logistic Regression', 'Passive Aggressive Classifier', 'Multinomial Naive Bayes'\n",
    "                                , 'Gaussian Naive Bayes', 'Bernoulli Naive Bayes'\n",
    "                                , 'Decision Tree', 'Random Forest', 'XGBoost']\n",
    "                                , 'Accuracy Scores': ['0.616', '0.507', '0.617', '0.302', '0.609', '0.467'\n",
    "                                                      , '0.576', '0.610']})\n",
    "\n",
    "\n",
    "table['Model'] = table['Model'].astype('category')\n",
    "table['Accuracy Scores'] = table['Accuracy Scores'].astype('float32')\n",
    "\n",
    "pd.pivot_table(table, index = ['Model']).sort_values(by = 'Accuracy Scores', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the table above, the the algorithms are sorted according to their accuracy. It must be noted that these algorithms are yet to be tuned. Hence, algorithms such as XGBoost are much more potential to have a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Algorithms (Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, I will be experimenting with combining the top few algorithms to see if it is possible to achieve an even higher accuracy without tuning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the combined algorithms are: 0.7793715846994536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble = VotingClassifier(estimators = [('MultinomialNB', MNB)\n",
    "                            , ('Logistic regression', LR)]\n",
    "                            , voting = 'soft', weights = [1.5,1]).fit(x_train, y_train)\n",
    "\n",
    "print('The accuracy for the combined algorithms are:',ensemble.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out different weights and mixing different algorithms, the best pair is the one above. As we can see, there is not much significant difference between the combined algorithms and the top few algorithms on their own. This only emphasises the importance of feature selection and data engineering as that is where the accuracy can be changed the most."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
